{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The DQN agent is implemented with both experience replay and fix target. It collects its rollout using the epsilon greedy algorithm.\n",
    "\n",
    "Hyper-parameters:\n",
    "eta = 0.99\n",
    "memory maxlen = 2000\n",
    "epsilon = 1\n",
    "epsilon maxs = 1\n",
    "epsilon mins = 0.01\n",
    "epsilon decay = 0.999\n",
    "optimizer: Adam(0.003)\n",
    "batch size = 64\n",
    "max steps = 500\n",
    "total episodes in training = 1500\n",
    "\n",
    "\n",
    "episodes_train = 300\n",
    "batch_size = 32\n",
    "ws = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from collections import deque\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import pickle\n",
    "\n",
    "\n",
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "def reshape(intiger):\n",
    "    return  np.reshape(intiger, [1, state_size])\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 64)\n",
    "        self.fc2 = nn.Linear(64, 64)\n",
    "        self.fc3 = nn.Linear(64, 64)\n",
    "        self.fc4 = nn.Linear(64, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.leaky_relu(self.fc1(x))\n",
    "        x = F.leaky_relu(self.fc2(x))\n",
    "        x = F.leaky_relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self, action_size,state_size):\n",
    "        self.eta = 0.99\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.DQN = QNetwork(state_size, action_size)\n",
    "        self.optimizer = optim.Adam(self.DQN.parameters(), lr=0.0001)\n",
    "        self.target_model_DQN = QNetwork(state_size, action_size)\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        \n",
    "        self.step = 0\n",
    "        self.target_update_freq = 20\n",
    "\n",
    "        self.epsilon = 1  # Exploration rate\n",
    "        self.min_eps = 0.01\n",
    "        self.eps_decay = 0.99\n",
    "        self.plotter = {'num_of_eps_til_475':[] , 'loss':[] , 'tot_rawrd_per_ep':[]}\n",
    "\n",
    "\n",
    "    def add_memory(self, new_state, reward, done, state, action):\n",
    "        self.memory.append((new_state, reward, done, state, action))\n",
    "\n",
    "    def update(self, batch_size):\n",
    "        \"\"\"update Q\n",
    "        batch size is the total repaly memory samples used\n",
    "        \"\"\"\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        if self.step % self.target_update_freq == 0:\n",
    "            self.target_model_DQN.load_state_dict(self.DQN.state_dict())\n",
    "            self.target_model_DQN.eval()\n",
    "\n",
    "        x_train = np.zeros((batch_size, self.state_size))\n",
    "        y_train = np.zeros((batch_size, self.action_size))\n",
    "        i = 0\n",
    "        for new_state, reward, done, state, action in minibatch:\n",
    "            S = reshape(state)\n",
    "            A = action\n",
    "            r = reward\n",
    "            S_next = reshape(new_state)\n",
    "            terminated = done\n",
    "\n",
    "            if terminated:\n",
    "                y = r\n",
    "            else:\n",
    "                y = r + self.eta * torch.max(self.target_model_DQN(torch.tensor(S_next, dtype=torch.float32)))\n",
    "\n",
    "            y_fit = self.DQN(torch.tensor(S, dtype=torch.float32)).detach().numpy()\n",
    "            y_fit[0][A] = y\n",
    "\n",
    "            x_train[i], y_train[i] = S, y_fit\n",
    "            i = i + 1\n",
    "\n",
    "        x_train_tensor = torch.tensor(x_train, dtype=torch.float32)\n",
    "        y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        output = self.DQN(x_train_tensor)\n",
    "        loss = F.mse_loss(output, y_train_tensor)\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # callback\n",
    "        self.plotter[\"loss\"].append(loss.item())\n",
    "\n",
    "        if self.epsilon > self.min_eps:\n",
    "            self.epsilon *= self.eps_decay\n",
    "        self.step += 1\n",
    "\n",
    "            \n",
    "    def get_action(self,state):\n",
    "        '''Choose A 0 from S 0 using policy derived from Q (e.g., \"-greedy)'''\n",
    "        z = np.random.choice(2, 1, p=[self.epsilon,1-self.epsilon])[0]\n",
    "        if z == 1:\n",
    "            with torch.no_grad():\n",
    "                state_tensor = torch.tensor(reshape(state), dtype=torch.float32)\n",
    "                action = torch.argmax(self.DQN(state_tensor)).item()\n",
    "        else:\n",
    "            action = np.random.randint(2)\n",
    "        return action\n",
    "\n",
    "\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = Agent(action_size, state_size)\n",
    "\n",
    "batch_size = 32\n",
    "episodes_train = 300\n",
    "\n",
    "# training\n",
    "for episode in tqdm(range(episodes_train)):\n",
    "\n",
    "    state, info = env.reset()\n",
    "    \n",
    "    exp_return = 0\n",
    "    while True:       \n",
    "\n",
    "        action = agent.get_action(state)\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "\n",
    "        agent.add_memory(new_state, reward, terminated, state, action)    \n",
    "        if len(agent.memory) > batch_size:\n",
    "            agent.update(batch_size)\n",
    "\n",
    "        state = new_state\n",
    "        \n",
    "        exp_return = exp_return + reward\n",
    "\n",
    "        if terminated or truncated:\n",
    "            break\n",
    "    \n",
    "    agent.plotter['tot_rawrd_per_ep'].append(exp_return)\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(agent.plotter['tot_rawrd_per_ep'])\n",
    "    plt.xlabel('episode')\n",
    "    plt.title('total raward per episode')\n",
    "    plt.savefig('Tot_rawrd_per_ep.png')\n",
    "    plt.close()\n",
    "\n",
    "    fig = plt.figure()\n",
    "    plt.plot(agent.plotter['loss'])\n",
    "    plt.xlabel('step')\n",
    "    plt.title('loss')\n",
    "    plt.savefig('loss.png')\n",
    "    plt.close()\n",
    "\n",
    "env.close() \n",
    "\n",
    "\n",
    "with open(\"result.json\", \"w\") as fp:\n",
    "    json.dump(agent.plotter, fp)\n",
    "\n",
    "with open(\"model.pkl\", \"wb\") as fp:\n",
    "    pickle.dump(agent, fp)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
